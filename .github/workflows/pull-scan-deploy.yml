
name: CI/CD -> SonarQube -> Trivy -> EKS -> Apply Manifests

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Docker Hub image tag (default: latest)"
        required: false
        default: "latest"
      fail_on_vulns:
        description: "Fail the pipeline if Trivy finds CRITICAL/HIGH (true/false)"
        type: boolean
        required: false
        default: true

# Prevent overlapping deploys on the same branch
concurrency:
  group: deploy-${{ github.ref }}
  cancel-in-progress: true

env:
  # ---- Image & K8s settings ----
  IMAGE_REPO: devenops641/endhunger
  IMAGE_TAG: latest
  K8S_NAMESPACE: default
  K8S_DEPLOYMENT: endhunger
  K8S_CONTAINER: endhunger

jobs:
  deploy:
    # If you added labels (e.g., linux, eks), use: runs-on: [ self-hosted, linux, eks ]
    runs-on: self-hosted

    steps:
      # 1) Checkout code
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2) Set IMAGE_TAG and TRIVY_EXIT_CODE based on inputs
      - name: Set image tag and Trivy fail mode
        shell: bash
        run: |
          TAG="${{ inputs.image_tag }}"
          if [ -z "$TAG" ]; then TAG="latest"; fi
          echo "IMAGE_TAG=$TAG" >> "$GITHUB_ENV"
          # fail_on_vulns: true -> exit-code=1 (block), false -> 0 (allow)
          if [ "${{ inputs.fail_on_vulns }}" = "true" ]; then
            echo "TRIVY_EXIT_CODE=1" >> "$GITHUB_ENV"
          else
            echo "TRIVY_EXIT_CODE=0" >> "$GITHUB_ENV"
          fi
          echo "Using image: ${{ env.IMAGE_REPO }}:$TAG"
          echo "Trivy exit code policy: ${{ env.TRIVY_EXIT_CODE }}"

      # 3) Pull Docker image (ensures the tag exists and speeds up Trivy)
      - name: Pull Docker image
        run: |
          docker pull ${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}

      # 4) SonarQube Scan (optional)
      - name: SonarQube Scan
        if: ${{ secrets.SONAR_HOST_URL != '' && secrets.SONAR_TOKEN != '' }}
        uses: SonarSource/sonarqube-scan-action@v2
        with:
          args: >
            -Dsonar.projectKey=endhunger
            -Dsonar.projectName=endhunger
            -Dsonar.sources=.
        env:
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}

      # 5) Cache Trivy DB (speeds up scans on self-hosted runner)
      - name: Cache Trivy DB
        uses: actions/cache@v4
        with:
          path: ~/.cache/trivy
          key: trivy-db-${{ runner.os }}
          restore-keys: |
            trivy-db-

      # 6) Trivy image vulnerability scan
      #    Toggle fail-open/fail-closed via workflow_dispatch 'fail_on_vulns'
      - name: Trivy Scan
        uses: aquasecurity/trivy-action@0.33.1
        with:
          image-ref: ${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}
          format: 'table'
          output: 'trivy-results.txt'
          exit-code: ${{ env.TRIVY_EXIT_CODE }}   # 1 = block, 0 = allow
          severity: 'CRITICAL,HIGH'
          ignore-unfixed: true                    # optional: ignore issues without fixes yet
      - name: Upload Trivy report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: trivy-report
          path: trivy-results.txt

      # 7) Install kubectl (action simply downloads kubectl; OK for EKS too)
      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.29.0'

      # 8) Configure AWS credentials (via secrets)
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      # 9) Update kubeconfig for EKS
      - name: Update kubeconfig
        run: |
          aws eks update-kubeconfig \
            --name ${{ secrets.EKS_CLUSTER_NAME }} \
            --region ${{ secrets.AWS_REGION }}
          kubectl version --short
          kubectl get ns

      # 10) Apply manifests (ensures resources exist/update declaratively)
      - name: Apply Kubernetes manifests
        run: |
          kubectl apply -n ${{ env.K8S_NAMESPACE }} -f k8s/deployment.yaml
          kubectl apply -n ${{ env.K8S_NAMESPACE }} -f k8s/service.yaml
          kubectl -n ${{ env.K8S_NAMESPACE }} get deploy ${{ env.K8S_DEPLOYMENT }}
          kubectl -n ${{ env.K8S_NAMESPACE }} get svc endhunger-lb

      # 11) Set image on the Deployment (pin to selected tag)
      - name: Update Deployment image
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} set image deployment/${{ env.K8S_DEPLOYMENT }} \
            ${{ env.K8S_CONTAINER }}=${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}

      # 12) Wait for rollout (fail if not healthy in 5 minutes)
      - name: Wait for rollout
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} rollout status deployment/${{ env.K8S_DEPLOYMENT }} --timeout=300s

      # 13) Verify resources (pods + service)
      - name: Verify Deployment and Pods
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} get deploy ${{ env.K8S_DEPLOYMENT }} -o wide
          kubectl -n ${{ env.K8S_NAMESPACE }} get pods -l app=${{ env.K8S_DEPLOYMENT }} -o wide

      # 14) Print External Load Balancer hostname
      - name: Verify Service and External LB
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} get svc endhunger-lb -o wide
          echo "External LB Hostname:"
          kubectl -n ${{ env.K8S_NAMESPACE }} get svc endhunger-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
