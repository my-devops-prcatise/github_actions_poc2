
name: CI/CD -> SonarQube -> (Trivy optional) -> EKS -> Apply Manifests

on:
  push:
    branches: [ "main" ]
  workflow_dispatch:
    inputs:
      image_tag:
        description: "Docker Hub image tag (default: latest)"
        required: false
        default: "latest"

# Prevent overlapping deploy runs on the same ref
concurrency:
  group: deploy-${{ github.ref }}
  cancel-in-progress: true

env:
  # ---- Image settings ----
  IMAGE_REPO: devenops641/endhunger
  IMAGE_TAG: latest

  # ---- Kubernetes settings ----
  K8S_NAMESPACE: default
  K8S_DEPLOYMENT: endhunger
  K8S_CONTAINER: endhunger

  # ---- Cluster settings (explicit to avoid secret mismatch) ----
  CLUSTER_NAME: endhunger-eks
  AWS_REGION: us-east-1

jobs:
  deploy:
    runs-on: self-hosted
    # If you registered labels on the runner, use:
    # runs-on: [ self-hosted, linux, eks ]

    # Expose Sonar secrets to env so we can test them in `if:` safely
    env:
      SONAR_HOST_URL: ${{ secrets.SONAR_HOST_URL }}
      SONAR_TOKEN:    ${{ secrets.SONAR_TOKEN }}

    steps:
      # 1) Checkout
      - name: Checkout repository
        uses: actions/checkout@v4

      # 2) Set IMAGE_TAG (from workflow_dispatch input)
      - name: Set IMAGE_TAG
        shell: bash
        run: |
          TAG="${{ github.event.inputs.image_tag }}"
          if [ -z "$TAG" ]; then TAG="latest"; fi
          echo "IMAGE_TAG=$TAG" >> "$GITHUB_ENV"
          echo "Using image: ${{ env.IMAGE_REPO }}:$TAG"

      # 3) Install kubectl
      - name: Install kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'v1.29.0'

      # 4) Pull Docker image (sanity check the tag exists)
      - name: Pull Docker image
        run: |
          echo "Pulling ${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}"
          docker pull ${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}

      # 5) SonarQube Scan (runs only if secrets are present)
      - name: SonarQube Scan
        if: ${{ env.SONAR_HOST_URL != '' && env.SONAR_TOKEN != '' }}
        uses: SonarSource/sonarqube-scan-action@v2
        with:
          args: >
            -Dsonar.projectKey=endhunger
            -Dsonar.projectName=endhunger
            -Dsonar.sources=.
        env:
          SONAR_TOKEN: ${{ env.SONAR_TOKEN }}
          SONAR_HOST_URL: ${{ env.SONAR_HOST_URL }}

      # 6) (Optional) Trivy â€” currently disabled
      # - name: Trivy Scan
      #   uses: aquasecurity/trivy-action@0.33.1
      #   with:
      #     image-ref: ${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}
      #     format: 'table'
      #     exit-code: '1'
      #     severity: 'CRITICAL,HIGH'

      # 7) Configure AWS credentials
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          # If you choose to assume a role, uncomment and set the ARN below,
          # and make sure that role ARN is mapped in aws-auth:
          # role-to-assume: arn:aws:iam::003364514867:role/<DeployRole>
          # role-session-name: gha-deploy

      # 8) Update kubeconfig + sanity checks (Fargate-friendly)
      - name: Update kubeconfig and cluster checks
        run: |
          set -e
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region "$AWS_REGION"
          kubectl version --client
          kubectl config current-context
          kubectl get ns

      # 9) Fail fast if not authorized in cluster
      - name: Authorization guard (fail fast)
        run: |
          set -e
          if ! kubectl auth can-i get pods -A; then
            echo "Not authorized to access cluster resources with current IAM identity."
            echo "Ensure your IAM role/user ARN is added to aws-auth ConfigMap."
            exit 1
          fi

      # 10) Install Kustomize (v5) if not present
      - name: Install Kustomize (v5)
        shell: bash
        run: |
          if ! command -v kustomize >/dev/null 2>&1; then
            curl -sL \
              https://github.com/kubernetes-sigs/kustomize/releases/download/kustomize%2Fv5.4.1/kustomize_v5.4.1_linux_amd64.tar.gz \
              | tar -xz
            sudo mv kustomize /usr/local/bin/kustomize
          fi
          kustomize version

      # 11) Deploy with Kustomize (overlays/prod)
      # Adjust the overlay path if needed (e.g., overlays/dev or overlays/staging)
      - name: Deploy with Kustomize
        run: |
          echo "Applying Kustomize overlay..."
          kustomize build k8s/overlays/prod | kubectl apply -f -
          kubectl -n ${{ env.K8S_NAMESPACE }} get deploy,svc,cm,ingress -o wide || true

      # 12) Apply manifests (fallback / hybrid approach)
      # Keep if you still maintain plain manifests; otherwise comment out.
      - name: Apply Kubernetes manifests
        run: |
          kubectl apply -n ${{ env.K8S_NAMESPACE }} -f k8s/deployment.yaml
          kubectl apply -n ${{ env.K8S_NAMESPACE }} -f k8s/service.yaml

      # 13) Set image on the Deployment (pin to selected tag)
      # If Kustomize already sets the image via patches, you can comment this out.
      - name: Update Deployment image
        run: |
          echo "Deploying image ${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }} to EKS"
          kubectl -n ${{ env.K8S_NAMESPACE }} set image deployment/${{ env.K8S_DEPLOYMENT }} \
            ${{ env.K8S_CONTAINER }}=${{ env.IMAGE_REPO }}:${{ env.IMAGE_TAG }}

      # 14) Wait for rollout
      - name: Wait for rollout
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} rollout status deployment/${{ env.K8S_DEPLOYMENT }} --timeout=300s

      # 15) Verify Deployment and Pods
      - name: Verify Deployment and Pods
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} get deploy ${{ env.K8S_DEPLOYMENT }} -o wide
          kubectl -n ${{ env.K8S_NAMESPACE }} get pods -l app=${{ env.K8S_DEPLOYMENT }} -o wide

      # 16) Verify Service and External LB
      - name: Verify Service and External LB
        run: |
          kubectl -n ${{ env.K8S_NAMESPACE }} get svc endhunger-lb -o wide
          echo "External LB Hostname:"
          kubectl -n ${{ env.K8S_NAMESPACE }} get svc endhunger-lb -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'; echo
